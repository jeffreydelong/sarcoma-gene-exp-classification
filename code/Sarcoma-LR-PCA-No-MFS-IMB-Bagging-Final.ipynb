{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189, 20605)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# load the PCA model\n",
    "sarcoma_df = pd.read_csv('Data/sarcoma-gene-exp-FPKM-zscore-no-label-nomfs.csv')\n",
    "sarcoma_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.51154087  0.09480786 -0.35176093 ...  0.51624723  0.02346097\n",
      "   2.48359169]\n",
      " [-0.19129576  0.15347405  0.20261954 ... -0.41028013  2.86123234\n",
      "   0.16777757]\n",
      " [ 0.26472818  2.327348   -0.56049386 ... -0.21651268  0.0192731\n",
      "   0.76332633]\n",
      " ...\n",
      " [-0.2652622  -0.48026337 -1.16285933 ... -0.36779173  0.24949394\n",
      "  -0.36985907]\n",
      " [ 0.24931652 -0.03363532 -0.97441342 ... -0.50605902 -1.23871739\n",
      "   0.28153212]\n",
      " [-0.07085013 -0.29522455 -0.59015045 ... -0.50644652 -0.01137879\n",
      "  -0.39862195]]\n"
     ]
    }
   ],
   "source": [
    "# convert df to array\n",
    "sarcoma_data = sarcoma_df.to_numpy()\n",
    "print(sarcoma_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA fit is complete\n"
     ]
    }
   ],
   "source": [
    "# Run PCA based on preserving variance\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(sarcoma_data)\n",
    "print(\"PCA fit is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA transformation is complete\n"
     ]
    }
   ],
   "source": [
    "X = pca.transform(sarcoma_data)\n",
    "print(\"PCA transformation is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced shape: (189, 150)\n"
     ]
    }
   ],
   "source": [
    "print(\"reduced shape: {}\".format(str(X.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in labels\n",
    "sarcoma_labels_df = pd.read_csv('Data/sarcoma-gene-exp-FPKM-labels-nomfs.csv')\n",
    "sarcoma_labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2 2 5 3 2 4 4 3 4 4 4 3 0 3 4 2 4 4 0 2 4 3 3 3 2 0 4 4 5 3 3 2 4 4 5 4\n",
      " 3 4 4 5 4 2 4 4 4 4 4 4 2 4 3 3 2 2 4 4 4 4 4 4 4 2 2 3 4 2 3 4 4 3 4 3 2\n",
      " 4 3 2 3 4 4 3 4 3 4 3 4 4 3 4 4 4 0 4 3 4 3 3 3 5 2 0 3 3 4 2 3 0 3 4 2 4\n",
      " 2 0 3 4 4 3 2 2 3 4 4 4 4 4 3 2 4 2 4 0 2 2 3 4 4 2 3 4 4 3 3 4 3 4 2 2 3\n",
      " 2 2 0 4 2 4 2 4 3 3 4 2 4 2 4 3 4 2 3 4 2 4 4 2 2 3 4 4 4 3 2 4 2 3 0 3 3\n",
      " 2 2 3 2]\n"
     ]
    }
   ],
   "source": [
    "# Convert label df to np array\n",
    "y_df = sarcoma_labels_df['label']\n",
    "y = y_df.to_numpy()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of unique values of the said array:\n",
      "[[ 0  2  3  4  5]\n",
      " [10 44 50 80  5]]\n"
     ]
    }
   ],
   "source": [
    "# Get a count of the unique values in each categories to make sure there are enough to support cross-validation\n",
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2,\n",
    "    stratify=y, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into development and validation sets\n",
    "X_dev, X_val, y_dev, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25,\n",
    "    stratify=y_train, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({3: 48, 4: 48, 2: 48, 0: 48, 5: 48})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE # doctest: +NORMALIZE_WHITESPACE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "#smote_enn = SMOTEENN(random_state=0)\n",
    "#smote_tomek = SMOTETomek(random_state=0)\n",
    "# sm = BorderlineSMOTE(sampling_strategy='not majority', k_neighbors=4, m_neighbors=4, random_state=42)\n",
    "sm = SMOTE(sampling_strategy='not majority', k_neighbors=2, random_state=42)\n",
    "#sm = SMOTE(sampling_strategy={0: 40, 1: 40, 2: 40, 3: 40, 4: 40}, random_state=42)\n",
    "#sm = SMOTE(sampling_strategy=.3, random_state=42)\n",
    "\n",
    "X_res, y_res = sm.fit_resample(X_dev, y_dev)\n",
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baggingClassifiers():\n",
    "    # This example is taken from https://github.com/ageron/handson-ml/blob/master/07_ensemble_learning_and_random_forests.ipynb \n",
    "\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    bag_clf = BaggingClassifier(\n",
    "        LogisticRegression(multi_class='multinomial', solver='sag', max_iter=2000, C=1.0, penalty='l2'), n_estimators=100,\n",
    "            max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "    bag_clf.fit(X_res, y_res)\n",
    "    y_pred = bag_clf.predict(X_test)\n",
    "    print('Bagging LR',accuracy_score(y_test, y_pred))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging LR 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "y_pred = baggingClassifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      1.00      1.00         2\n",
      "     Class 1       1.00      1.00      1.00         9\n",
      "     Class 2       1.00      1.00      1.00        10\n",
      "     Class 3       1.00      0.94      0.97        16\n",
      "     Class 4       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.90      0.99      0.93        38\n",
      "weighted avg       0.99      0.97      0.98        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class_names = [\"Class {}\".format(i) for i in range(5)]\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  0,  0,  0,  0],\n",
       "       [ 0,  9,  0,  0,  0],\n",
       "       [ 0,  0, 10,  0,  0],\n",
       "       [ 0,  0,  0, 15,  1],\n",
       "       [ 0,  0,  0,  0,  1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
